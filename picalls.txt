nach dem sync gram gucken

ac = pi.pdtype.sample_placeholder([None])
oldpi.pd.kl(pi.pd)
pi.pd.entropy()
pi.pd.logp(ac) - oldpi.pd.logp(ac)
pi.get_trainable_variables()
pi.vpred
ac, vpred = pi.act(stochastic, ob)


    def get_vars(self):
        res=[];
        for i in range(len(self.W_conv)):
            for j in range(len(self.W_conv[0])):
                if(self.fixed_path[i,j]==0.0):
                    res+=[self.W_conv[i,j]]+[self.b_conv[i,j]];
        for i in range(len(self.W_lin)):
            if(self.fixed_path[-1,i]==0.0):
                res+=[self.W_lin[i]]+[self.b_lin[i]];
        # if (self.training_stage == 0):
            # res+=[self.W_fc2_source]+[self.b_fc2_source];
            # res+=[self.W_fc3]+[self.b_fc3];
        res+=[self.W_fc2]+[self.b_fc2];
        res+=[self.W_fc3]+[self.b_fc3];
        return res;


vars_=training_thread.local_network.get_vars()

training_thread.set_training_stage(task)
training_thread.local_network.set_fixed_path(fixed_path)
training_thread.set_start_time(start_time)
diff_global_t = training_thread.process(sess, sess.run([global_step])[0], "",  summary_op, "",score_ph,score_ops,"",FLAGS,score_set_ph[FLAGS.task_index],score_set_ops[FLAGS.task_index])
pathnet.geopath_insert(sess,training_thread.local_network.geopath_update_placeholders_set[i],training_thread.local_network.geopath_update_ops_set[i],tmp,FLAGS.L,FLAGS.M)
training_thread.local_network.set_fixed_path(fixed_path)
vars_idx=training_thread.local_network.get_vars_idx()

assign_old_eq_new()  # set old parameter values to new parameter values
loss_names
lossandgrad
compute_losses
loss_names


        # Setup losses and stuff
        # ----------------------------------------
        atarg = tf.placeholder(dtype=tf.float32, shape=[None])  # Target advantage function (if applicable)
        ret = tf.placeholder(dtype=tf.float32, shape=[None])  # Empirical return

        lrmult = tf.placeholder(name='lrmult', dtype=tf.float32,
                                shape=[])  # learning rate multiplier, updated with schedule
        clip_param = clip_param * lrmult  # Annealed cliping parameter epislon

        ob = U.get_placeholder_cached(name="ob")
        ac = pi.pdtype.sample_placeholder([None])

        kloldnew = oldpi.pd.kl(pi.pd)
        ent = pi.pd.entropy()
        meankl = tf.reduce_mean(kloldnew)
        meanent = tf.reduce_mean(ent)
        pol_entpen = (-entcoeff) * meanent

        ratio = tf.exp(pi.pd.logp(ac) - oldpi.pd.logp(ac))  # pnew / pold
        surr1 = ratio * atarg  # surrogate from conservative policy iteration
        surr2 = tf.clip_by_value(ratio, 1.0 - clip_param, 1.0 + clip_param) * atarg  #
        pol_surr = - tf.reduce_mean(tf.minimum(surr1, surr2))  # PPO's pessimistic surrogate (L^CLIP)
        vf_loss = tf.reduce_mean(tf.square(pi.vpred - ret))
        total_loss = pol_surr + pol_entpen + vf_loss
        losses = [pol_surr, pol_entpen, vf_loss, meankl, meanent]
        loss_names = ["pol_surr", "pol_entpen", "vf_loss", "kl", "ent"]

        var_list = pi.get_trainable_variables()
        lossandgrad = U.function([ob, ac, atarg, ret, lrmult], losses + [U.flatgrad(total_loss, var_list)])
        adam = MpiAdam(var_list, epsilon=adam_epsilon)

        assign_old_eq_new = U.function([], [], updates=[tf.assign(oldv, newv)
                                                        for (oldv, newv) in
                                                        zipsame(oldpi.get_variables(), pi.get_variables())])
        compute_losses = U.function([ob, ac, atarg, ret, lrmult], losses)